{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb85ffd4",
   "metadata": {},
   "source": [
    "### Initialize Required Classes, Methods, and Functions\n",
    "\n",
    "Required installations: Pygame, Gym, Tensorflow, SciPy, NumPy, Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c827aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "#This block initializes the environment and A2C Agents.\n",
    "\n",
    "class GymEnvironment(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(GymEnvironment, self).__init__()\n",
    "\n",
    "        # Define the screen dimensions\n",
    "        self.screen_width = 400\n",
    "        self.screen_height = 300\n",
    "\n",
    "        # Define the action space (up, down, left, right, none)\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "\n",
    "        # Define the observation space\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(self.screen_height, self.screen_width, 3), dtype=np.uint8)\n",
    "\n",
    "        # Initialize Pygame\n",
    "        pygame.init()\n",
    "\n",
    "        # Create the game screen\n",
    "        self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "        pygame.display.set_caption(\"Gym Environment\")\n",
    "\n",
    "        # Initialize game state variables\n",
    "        self.player_size = 10\n",
    "        self.player_x = self.screen_width // 2\n",
    "        self.player_y = self.screen_height // 2\n",
    "\n",
    "        # Enemy related variables\n",
    "        self.enemies = []\n",
    "        self.enemy_spawn_interval = 3.0  # In seconds\n",
    "        self.enemy_speed = 3  # Pixels moved per enemy update\n",
    "        self.enemy_lifetime = 20.0  # In seconds\n",
    "        self.max_time = 20.0  # Maximum time allowed for the game (in seconds)\n",
    "        self.start_time = 0.0  # Keep track of the starting time\n",
    "        self.last_enemy_spawn_time = time.time()\n",
    "\n",
    "        # Scoring related variables\n",
    "        self.score = 0\n",
    "        self.last_score_time = time.time()\n",
    "\n",
    "        # Powerup related variables\n",
    "        self.powerup_active = False\n",
    "        self.powerup_spawn_interval = 3.0  # In seconds\n",
    "        self.last_powerup_spawn_time = time.time()\n",
    "        self.powerup_x = 0\n",
    "        self.powerup_y = 0\n",
    "            \n",
    "    def reset(self):\n",
    "        # Reset the game state\n",
    "        self.player_x = self.screen_width // 2\n",
    "        self.player_y = self.screen_height // 2\n",
    "        self.enemies = []\n",
    "        self.last_enemy_spawn_time = time.time()\n",
    "        self.enemy_spawn_interval = 3.0  # In seconds\n",
    "        self.enemy_speed = 3  # Pixels moved per enemy update\n",
    "        self.score = 0\n",
    "        self.last_score_time = time.time()\n",
    "        self.powerup_active = False\n",
    "        self.last_powerup_spawn_time = time.time()\n",
    "        self.start_time = time.time()\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # Capture the current game screen as the observation\n",
    "        screen_surface = pygame.display.get_surface()\n",
    "        observation = pygame.surfarray.array3d(screen_surface)\n",
    "        observation = np.transpose(observation,(1,0,2))\n",
    "        return observation\n",
    "\n",
    "    def _spawn_enemy(self):\n",
    "        enemy_size = random.uniform(0.5, 1.5) * self.player_size\n",
    "        enemy_x = random.randint(0, self.screen_width - 1)\n",
    "        enemy_y = random.randint(0, self.screen_height - 1)\n",
    "        enemy_spawn_time = time.time()\n",
    "        self.enemies.append((enemy_x, enemy_y, enemy_size, enemy_spawn_time))\n",
    "\n",
    "    def _move_enemies(self, episode):\n",
    "        for i, (enemy_x, enemy_y, enemy_size, enemy_spawn_time) in enumerate(self.enemies):\n",
    "            # Calculate distance between enemy and player\n",
    "            distance_to_player = np.sqrt((self.player_x - enemy_x)**2 + (self.player_y - enemy_y)**2)\n",
    "            speed = self.enemy_speed\n",
    "            \n",
    "            # Enemies only seek player if episode count is 25 or greater, otherwise enemies move randomly.\n",
    "            if episode > 24:\n",
    "                # Enemies move towards the player if they are within a certain distance\n",
    "                if distance_to_player < 1000:  \n",
    "                    # Calculate direction vector towards the player\n",
    "                    dx = self.player_x - enemy_x\n",
    "                    dy = self.player_y - enemy_y\n",
    "\n",
    "                    # Move the enemy one step towards the player in the cardinal direction\n",
    "                    if abs(dx) >= abs(dy):\n",
    "                        self.enemies[i] = (enemy_x + speed * np.sign(dx), enemy_y, enemy_size, enemy_spawn_time)\n",
    "                    else:\n",
    "                        self.enemies[i] = (enemy_x, enemy_y + speed * np.sign(dy), enemy_size, enemy_spawn_time)\n",
    "            elif episode <= 24:\n",
    "                direction = random.choice(['up', 'down', 'left', 'right'])\n",
    "                if direction == 'up':\n",
    "                    self.enemies[i] = (enemy_x, max(0, enemy_y - speed), enemy_size, enemy_spawn_time)\n",
    "                elif direction == 'down':\n",
    "                    self.enemies[i] = (enemy_x, min(self.screen_height - 1, enemy_y + speed), enemy_size, enemy_spawn_time)\n",
    "                elif direction == 'left':\n",
    "                    self.enemies[i] = (max(0, enemy_x - speed), enemy_y, enemy_size, enemy_spawn_time)\n",
    "                elif direction == 'right':\n",
    "                    self.enemies[i] = (min(self.screen_width - 1, enemy_x + speed), enemy_y, enemy_size, enemy_spawn_time)\n",
    "    \n",
    "    def _is_colliding(self, x1, y1, size1, x2, y2, size2, buffer=5):\n",
    "        return (x1 + size1 + buffer > x2 and x1 < x2 + size2 + buffer and\n",
    "                y1 + size1 + buffer > y2 and y1 < y2 + size2 + buffer)\n",
    "\n",
    "    def _pick_up_powerup(self):\n",
    "\n",
    "        if self.powerup_active and self._is_colliding(self.player_x, self.player_y, self.player_size, self.powerup_x, self.powerup_y, 10):\n",
    "            self.score *= 2\n",
    "            self.powerup_active = False\n",
    "\n",
    "            # Increase enemy speed and decrease enemy spawn interval\n",
    "            self.enemy_speed *= 1.5\n",
    "            self.enemy_spawn_interval = max(0.01, self.enemy_spawn_interval - 0.5)\n",
    "\n",
    "\n",
    "    def _spawn_powerup(self):\n",
    "        self.powerup_x = random.randint(0, self.screen_width - 1)\n",
    "        self.powerup_y = random.randint(0, self.screen_height - 1)\n",
    "        self.powerup_active = True\n",
    "\n",
    "    def _remove_old_enemies(self):\n",
    "        current_time = time.time()\n",
    "        self.enemies = [(x, y, size, spawn_time) for x, y, size, spawn_time in self.enemies\n",
    "                        if current_time - spawn_time <= self.enemy_lifetime]\n",
    "\n",
    "    def _calculate_reward_boost(self):\n",
    "        if self.powerup_active:\n",
    "            distance_to_powerup = np.sqrt((self.player_x - self.powerup_x)**2 + (self.player_y - self.powerup_y)**2)\n",
    "            max_distance = np.sqrt((self.screen_width - 1)**2 + (self.screen_height - 1)**2)  # Max distance on the screen\n",
    "            normalized_distance = distance_to_powerup / max_distance\n",
    "            return 2.0 - 2*normalized_distance  # The closer to the powerup, the higher the reward boost\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "    def step(self, action, episode):\n",
    "        # Execute the specified action from the allowed 5\n",
    "        if action == 0:  # Up\n",
    "            self.player_y = max(0, self.player_y - 5)\n",
    "        elif action == 1:  # Down\n",
    "            self.player_y = min(self.screen_height - self.player_size, self.player_y + 5)\n",
    "        elif action == 2:  # Left\n",
    "            self.player_x = max(0, self.player_x - 5)\n",
    "        elif action == 3:  # Right\n",
    "            self.player_x = min(self.screen_width - self.player_size, self.player_x + 5)\n",
    "        else:  # No movement\n",
    "            pass\n",
    "        \n",
    "        # Calculate the power up proximity bonus\n",
    "        reward_boost = self._calculate_reward_boost()\n",
    "\n",
    "        # Update the score\n",
    "        self.score = self.score + 1 + reward_boost\n",
    "        \n",
    "        # Spawn new enemies periodically if Episode count is 10 or greater\n",
    "        current_time = time.time()\n",
    "        \n",
    "        if episode > 9:\n",
    "            if current_time - self.last_enemy_spawn_time > self.enemy_spawn_interval:\n",
    "                self._spawn_enemy()\n",
    "                self.last_enemy_spawn_time = current_time\n",
    "\n",
    "        # Spawn powerup periodically\n",
    "        if not self.powerup_active and current_time - self.last_powerup_spawn_time > self.powerup_spawn_interval:\n",
    "            self._spawn_powerup()\n",
    "            self.last_powerup_spawn_time = current_time\n",
    "\n",
    "        # Move enemies\n",
    "        self._move_enemies(episode)\n",
    "\n",
    "        # Check if player picks up the powerup\n",
    "        self._pick_up_powerup()\n",
    "        \n",
    "        # Remove old enemies\n",
    "        self._remove_old_enemies()\n",
    "\n",
    "        # Check for collisions between the player and enemies\n",
    "        for enemy_x, enemy_y, enemy_size, _ in self.enemies:\n",
    "            if self._is_colliding(self.player_x, self.player_y, self.player_size, enemy_x, enemy_y, enemy_size):\n",
    "                self.score -= round(self.score / 2,0)\n",
    "                return self._get_observation(), self.score, True, {'score': self.score}\n",
    "\n",
    "        # Render the screen with the updated player, enemy, and powerup positions\n",
    "        self.screen.fill((0, 0, 0))\n",
    "        pygame.draw.rect(self.screen, (255, 255, 255), (self.player_x, self.player_y, self.player_size, self.player_size))\n",
    "        for enemy_x, enemy_y, enemy_size, _ in self.enemies:\n",
    "            pygame.draw.rect(self.screen, (255, 0, 0), (enemy_x, enemy_y, enemy_size, enemy_size))\n",
    "        if self.powerup_active:\n",
    "            pygame.draw.rect(self.screen, (0, 255, 0), (self.powerup_x, self.powerup_y, 10, 10))\n",
    "        pygame.display.flip()\n",
    "\n",
    "        # Get the current observation\n",
    "        observation = self._get_observation()\n",
    "\n",
    "        # Calculate the reward\n",
    "        reward = self.score\n",
    "        # Check if the game is done due to reaching the time limit\n",
    "        if current_time-self.start_time >= self.max_time:\n",
    "            return self._get_observation(), self.score, True, {'score': self.score, 'time_elapsed': current_time-self.start_time}\n",
    "\n",
    "        # Check if the game is done\n",
    "        done = False\n",
    "\n",
    "        return observation, reward, done, {'score': self.score}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Required render method for Gym environments\n",
    "        if mode == 'human':\n",
    "            pygame.display.update()\n",
    "\n",
    "    def close(self):\n",
    "        # Close the Pygame window\n",
    "        pygame.quit()\n",
    "\n",
    "class A2CAgent:\n",
    "    def __init__(self, state_shape, action_size, epsilon = 0.1, learning_rate_actor=0.001, learning_rate_critic=0.005, gamma=0.99):\n",
    "        # Initialize the actor and critic neural networks\n",
    "        self.actor = self.build_actor_network(state_shape, action_size)\n",
    "        self.critic = self.build_critic_network(state_shape)\n",
    "\n",
    "        # Define the optimizers\n",
    "        self.optimizer_actor = optimizers.Adam(learning_rate=learning_rate_actor)\n",
    "        self.optimizer_critic = optimizers.Adam(learning_rate=learning_rate_critic)\n",
    "\n",
    "        # Set the discount factor (gamma)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Set the epsilon factor\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build_actor_network(self, state_shape, action_size):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=state_shape))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(32, activation='relu'))\n",
    "        model.add(layers.Dense(action_size, activation='softmax'))\n",
    "        return model\n",
    "\n",
    "    def build_critic_network(self, state_shape):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=state_shape))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(32, activation='relu'))\n",
    "        model.add(layers.Dense(1, activation='linear'))\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state, env):\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        action_probs = self.actor.predict(state, verbose=0)[0]\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice(len(action_probs))\n",
    "        else:\n",
    "            # Check the validity of the action (ensure player stays on the screen)\n",
    "            valid_actions = []\n",
    "            for a in range(len(action_probs)):\n",
    "                player_x, player_y = env.player_x, env.player_y\n",
    "                if a == 0:  # Up\n",
    "                    player_y = max(0, player_y - 5)\n",
    "                elif a == 1:  # Down\n",
    "                    player_y = min(env.screen_height - env.player_size, player_y + 5)\n",
    "                elif a == 2:  # Left\n",
    "                    player_x = max(0, player_x - 5)\n",
    "                elif a == 3:  # Right\n",
    "                    player_x = min(env.screen_width - env.player_size, player_x + 5)\n",
    "                # Check if the new player position is different from the original position\n",
    "                if (player_x, player_y) != (env.player_x, env.player_y):\n",
    "                    valid_actions.append(a)\n",
    "\n",
    "            # If all moves lead to the player staying in the same position, choose a random action\n",
    "            if not valid_actions:\n",
    "                action = np.random.choice(len(action_probs))\n",
    "            else:\n",
    "                action = np.random.choice(valid_actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def compute_advantages(self, rewards, values, dones):\n",
    "        n = len(rewards)\n",
    "        advantages = np.zeros_like(rewards)\n",
    "\n",
    "        last_advantage = 0\n",
    "        for t in reversed(range(n)):\n",
    "            if dones[t]:\n",
    "                last_advantage = 0\n",
    "            delta = rewards[t] + self.gamma * values[t + 1] * (1 - int(dones[t])) - values[t]\n",
    "            last_advantage = delta + self.gamma * last_advantage * (1 - int(dones[t]))\n",
    "            advantages[t] = last_advantage\n",
    "\n",
    "        return advantages\n",
    "\n",
    "    def train(self, states, actions, advantages, targets):\n",
    "        with tf.GradientTape() as tape_actor, tf.GradientTape() as tape_critic:\n",
    "            # Actor loss (policy gradient)\n",
    "            action_probs = self.actor(states, training=True)\n",
    "            action_mask = tf.one_hot(actions, depth=action_probs.shape[1])\n",
    "            log_probs = tf.math.log(tf.reduce_sum(action_probs * action_mask, axis=1))\n",
    "            actor_loss = -tf.reduce_mean(log_probs * advantages)\n",
    "\n",
    "            # Critic loss (mean squared error)\n",
    "            values = self.critic(states, training=True)\n",
    "            critic_loss = tf.reduce_mean(tf.square(targets - values))\n",
    "\n",
    "        # Update actor and critic weights\n",
    "        gradients_actor = tape_actor.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        gradients_critic = tape_critic.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        self.optimizer_actor.apply_gradients(zip(gradients_actor, self.actor.trainable_variables))\n",
    "        self.optimizer_critic.apply_gradients(zip(gradients_critic, self.critic.trainable_variables))\n",
    "        \n",
    "    def save_model(self, actor_file, critic_file, total_episodes):\n",
    "        \n",
    "        # Create unique filenames for actor and critic model weights\n",
    "        actor_file = f'{actor_file}_episodes_{total_episodes}.h5'\n",
    "        critic_file = f'{critic_file}_episodes_{total_episodes}.h5'\n",
    "        \n",
    "        # Save the actor model weights\n",
    "        self.actor.save_weights(actor_file)\n",
    "\n",
    "        # Save the critic model weights\n",
    "        self.critic.save_weights(critic_file)\n",
    "    \n",
    "    def load_model(self, actor_file, critic_file):\n",
    "        # Load the actor model weights\n",
    "        self.actor.load_weights(actor_file)\n",
    "\n",
    "        # Load the critic model weights\n",
    "        self.critic.load_weights(critic_file)\n",
    "\n",
    "        \n",
    "# The function below tests fully trained models.\n",
    "def test_agent(env, agent, total_episodes=100):\n",
    "    # Load the trained agent's model weights\n",
    "    actor_file = 'actor_model_weights.h5_episodes_100.h5'  # Change this filename accordingly\n",
    "    critic_file = 'critic_model_weights.h5_episodes_100.h5'  # Change this filename accordingly\n",
    "    agent.actor.load_weights(actor_file)\n",
    "    agent.critic.load_weights(critic_file)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for episode in range(total_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.get_action(state, env)\n",
    "            state, reward, done, _ = env.step(action, 100)\n",
    "            total_reward += reward\n",
    "\n",
    "        results.append(total_reward)\n",
    "        print(f\"Test Episode {episode + 1}/{total_episodes}, Total Reward: {total_reward}\")\n",
    "\n",
    "    return results     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1e7ab",
   "metadata": {},
   "source": [
    "### Train a New Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614142e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment and the A2C agent\n",
    "env = GymEnvironment()\n",
    "state_shape = env.observation_space.shape\n",
    "action_size = env.action_space.n\n",
    "agent = A2CAgent(state_shape, action_size)\n",
    "\n",
    "# Training loop\n",
    "total_episodes = 100 #This can be changed based on training requirements\n",
    "for episode in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    states, actions, rewards, dones, values = [], [], [], [], []\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(state, env)\n",
    "        next_state, reward, done, _ = env.step(action, episode)\n",
    "\n",
    "        # Store the transition\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        values.append(agent.critic.predict(np.expand_dims(state, axis=0), verbose =0)[0][0])\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    # Compute the value targets and advantages\n",
    "    next_value = 0 if done else agent.critic.predict(np.expand_dims(state, axis=0), verbose =0)[0][0]\n",
    "    advantages = agent.compute_advantages(rewards, values + [next_value], dones)\n",
    "    value_targets = rewards + agent.gamma * np.array(values[1:] + [next_value]) * (1 - np.array(dones))\n",
    "\n",
    "    # Convert data to numpy arrays\n",
    "    states = np.array(states)\n",
    "    actions = np.array(actions)\n",
    "    advantages = np.array(advantages)\n",
    "    value_targets = np.array(value_targets)\n",
    "\n",
    "    # Train the agent\n",
    "    agent.train(states, actions, advantages, value_targets)\n",
    "\n",
    "    # Print the episode results\n",
    "    print(f\"Episode {episode + 1}/{total_episodes}, Total Reward: {total_reward}\")\n",
    "\n",
    "# Save the trained agent\n",
    "actor_file = 'actor_model_weights.h5'\n",
    "critic_file = 'critic_model_weights.h5'\n",
    "agent.save_model(actor_file, critic_file, total_episodes)\n",
    "    \n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a973e8d0",
   "metadata": {},
   "source": [
    "### Test a Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24a1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment and the A2C agent\n",
    "env = GymEnvironment()\n",
    "state_shape = env.observation_space.shape\n",
    "action_size = env.action_space.n\n",
    "agent = A2CAgent(state_shape, action_size)\n",
    "\n",
    "# Test the trained agent in 100 games and record the results\n",
    "test_results = test_agent(env, agent, total_episodes=100)\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10dbf08",
   "metadata": {},
   "source": [
    "### Run an Untrained Agent\n",
    "Note: due to the way the game is built, this will produce scores that are incongruent to trained agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d736d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = GymEnvironment()\n",
    "    game_score = []\n",
    "    total_reward = 0\n",
    "\n",
    "    # Test the environment for 100 games\n",
    "    while len(game_score) <= 99:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action, 100)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            game_score.append(total_reward)\n",
    "            total_reward = 0\n",
    "            env.reset()\n",
    "            \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caef868",
   "metadata": {},
   "source": [
    "### Roll Results into a DataFrame\n",
    "Note: This will require training a variety of different models. CSVs generated from training will be provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4099cfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df10 = pd.read_csv('10_episodes.csv') #Can change filenames as needed\n",
    "df20 = pd.read_csv('20_episodes.csv')\n",
    "df50 = pd.read_csv('50_episodes.csv')\n",
    "df100 = pd.read_csv('100_episodes.csv')\n",
    "\n",
    "all_results = pd.concat([df10,df20,df50,df100],axis=1)\n",
    "all_results = all_results.drop('Unnamed: 0', axis = 1)\n",
    "all_results.columns = ['10 Episodes','20 Episodes','50 Episodes','100 Episodes'] #Can change column names as needed\n",
    "df_eps = pd.DataFrame([i+1 for i in range(100)])\n",
    "all_results.insert(0,'Episode',df_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f54fa92",
   "metadata": {},
   "source": [
    "### Line Plot of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27191a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(all_results['Episode'],all_results['10 Episodes'], label = '10 Episodes')\n",
    "plt.plot(all_results['Episode'],all_results['20 Episodes'], label = '20 Episodes')\n",
    "plt.plot(all_results['Episode'],all_results['50 Episodes'], label = '50 Episodes')\n",
    "plt.plot(all_results['Episode'],all_results['100 Episodes'], label = '100 Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Episode Rewards')\n",
    "plt.title('Trained Agent Rewards over 100 Episodes Played')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0422e1d8",
   "metadata": {},
   "source": [
    "### Bar Plot of Means of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c570ed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = pd.DataFrame([np.mean(all_results['10 Episodes']),\n",
    "                      np.mean(all_results['20 Episodes']),\n",
    "                      np.mean(all_results['50 Episodes']),\n",
    "                      np.mean(all_results['100 Episodes'])])\n",
    "xlbl = ['10 Episodes','20 Episodes','50 Episodes','100 Episodes']\n",
    "\n",
    "plt.bar(xlbl,means[0])\n",
    "plt.ylabel('Mean Total Reward')\n",
    "plt.title('Mean Rewards over 100 Episodes for Each Training Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7114231",
   "metadata": {},
   "source": [
    "### Two-tailed T-tests Across All Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159c09b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def perform_ttest(column1, column2):\n",
    "    stat, p = stats.ttest_ind(column1, column2)\n",
    "    print(f\"T-test result - p-value: {p}\")\n",
    "\n",
    "for i in range(1,len(all_results.columns)):\n",
    "    for j in range(i + 1, len(all_results.columns)):\n",
    "        column1 = all_results[all_results.columns[i]]\n",
    "        column2 = all_results[all_results.columns[j]]\n",
    "        print(f\"Performing t-test between '{all_results.columns[i]}' and '{all_results.columns[j]}':\")\n",
    "        perform_ttest(column1, column2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
